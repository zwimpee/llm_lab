{
  "model_name": "meta-llama/Llama-2-7b-chat-hf",
  "model_type": "SequenceClassification",
  "num_labels": 4,
  "label_mapping": {"A": 0, "B": 1, "C": 2, "D": 3},
  "max_length": 4096,
  "pad_to_multiple_of": 8,
  "peft": {
    "lora": {
      "lora_alpha": 16,
      "lora_dropout": 0.05,
      "peft_type": "LORA", 
      "r": 8,
      "target_modules": [
        "q_proj", 
        "v_proj"
      ],
      "task_type": "SEQ_CLS" 
    } 
  },
  "dataset": {
    "name": "cais/mmlu",
    "type": "hf",
    "task_type": "text-classification",
    "splits": {
      "train_split": "auxiliary_train",
      "eval_split": "validation",
      "test_split": "test"
    },
    "category": ["all"],
    "max_train_examples": false,
    "max_eval_examples": false,
    "max_test_examples": false 
  },
  "metrics": ["accuracy"],
  "training_args": {
    "report_to": "wandb",
    "run_name": "llm_lab_20240314",
    "output_dir": "/opt/data/models/meta-llama/Llama-2-7b-chat-hf/mmlu/",
    "overwrite_output_dir": true,
    "save_strategy": "steps",
    "save_steps": 500,
    "logging_steps": 100,
    "logging_dir": "/opt/data/logs/",
    "logging_strategy": "steps",
    "logging_first_step": false,
    "logging_nan_inf_filter": true,
    "do_train": true,
    "max_steps": 100000,
    "do_eval": true,
    "evaluation_strategy": "steps",
    "eval_steps": 10000,
    "eval_delay": 0,
    "do_predict": false,
    "per_device_train_batch_size": 1,
    "per_device_eval_batch_size": 1,
    "gradient_accumulation_steps": 1,
    "learning_rate": 5e-5,
    "weight_decay": 0.0,
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "adam_epsilon": 1e-8,
    "max_grad_norm": 1.0,
    "num_train_epochs": 3.0,
    "lr_scheduler_type": "linear",
    "lr_scheduler_kwargs": {},
    "warmup_ratio": 0,
    "warmup_steps": 0,
    "seed": 42
  }
}